#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3 ëª¨ë¸ì„ ì‚¬ìš©í•œ í•œêµ­ì–´ ë²ˆì—­ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
ì—¬ëŸ¬ ëª¨ë¸ í¬ê¸°ë¥¼ ì§€ì›í•˜ë©°, JSONL ë°ì´í„°ë¡œ LoRA íŒŒì¸íŠœë‹ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

í‰ê°€ëŠ” ë³„ë„ì˜ eval_qwen3_translation.py ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import json
import os
import torch
from datasets import Dataset
from unsloth import FastLanguageModel, is_bfloat16_supported
from trl import SFTTrainer
from transformers import TrainingArguments
from typing import List, Dict
import argparse


# ==================== ì„¤ì • ====================
SYSTEM_PROMPT = (
    "You are an expert Korean translator.\n"
    "You translate English sentences into natural Korean, adapting to Korean internet and youth culture expressions.\n"
    "Do not translate word-for-word; preserve meaning and tone in Korean."
)

ALPACA_PROMPT = """{system_prompt}

### Instruction:
{instruction}

### Input:
{input}

### Output:
{output}"""

# ì§€ì›í•˜ëŠ” ëª¨ë¸ ëª©ë¡ (ì‹¤ì œ Hugging Faceì— ì¡´ì¬í•˜ëŠ” ëª¨ë¸)
AVAILABLE_MODELS = {
    "0.6B": "Qwen/Qwen3-0.6B",
    "1.7B": "Qwen/Qwen3-1.7B",  # 1.5B ëŒ€ì‹  1.7B ì‚¬ìš©
    "4B": "Qwen/Qwen3-4B",
    "8B": "Qwen/Qwen3-8B",  # 7B ëŒ€ì‹  8B ì‚¬ìš©
    "32B": "Qwen/Qwen3-32B",
}

# ==================== ë°ì´í„° ë¡œë“œ ====================
def load_dataset_jsonl(file_path: str) -> Dataset:
    """JSONL í˜•ì‹ì˜ ë°ì´í„° íŒŒì¼ì„ ë¡œë“œ"""
    data = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        data.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue
        print(f"âœ… ë°ì´í„° {len(data)}ê°œ ë¡œë“œ ì™„ë£Œ. ({file_path})")
    except FileNotFoundError:
        raise FileNotFoundError(f"âŒ '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    if not data:
        raise ValueError("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    return Dataset.from_list(data)


def sample_dataset(
    dataset: Dataset,
    sample_size: int = 50,
    random_seed: int = 42
) -> Dataset:
    """ë°ì´í„°ì…‹ì—ì„œ ì§€ì • ê°œìˆ˜ë§Œí¼ ë¬´ì‘ìœ„ ìƒ˜í”Œë§"""
    import random
    
    n = len(dataset)
    if n == 0:
        raise ValueError("âŒ ìƒ˜í”Œë§í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    if sample_size >= n:
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ìƒ˜í”Œ {sample_size}ê°œ ìš”ì²­, ë°ì´í„° {n}ê°œ â†’ ì „ì²´ ì‚¬ìš©")
        return dataset
    
    random.seed(random_seed)
    indices = random.sample(range(n), sample_size)
    sampled = dataset.select(indices)
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ì…‹ ìƒ˜í”Œë§ ì™„ë£Œ: {len(sampled)} / {n}ê°œ (seed={random_seed})")
    return sampled


def save_results_to_json(results: List[Dict], path: str) -> None:
    """í‰ê°€ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {path}")


def formatting_prompts_func(examples, tokenizer):
    """ë°ì´í„°ì…‹ í¬ë§·íŒ… í•¨ìˆ˜"""
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]
    texts = []
    
    EOS_TOKEN = tokenizer.eos_token
    
    for instruction, input_text, output in zip(instructions, inputs, outputs):
        text = ALPACA_PROMPT.format(
            system_prompt=SYSTEM_PROMPT,
            instruction=instruction,
            input=input_text,
            output=output
        ) + EOS_TOKEN
        texts.append(text)
    
    return {"text": texts}


# ==================== ëª¨ë¸ ë¡œë“œ ====================
def load_model(
    model_name: str,
    max_seq_length: int = 2048,
    dtype=None,
    load_in_4bit: bool = True,
    use_lora: bool = True,
    lora_r: int = 16,
    lora_alpha: int = 16,
    lora_dropout: float = 0.0,
):
    """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    try:
        print(f"\nğŸ”„ {model_name} ëª¨ë¸ ë¡œë“œ ì¤‘...")
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_name,
            max_seq_length=max_seq_length,
            dtype=dtype,
            load_in_4bit=load_in_4bit,
        )
        print(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
        if use_lora:
            print("ğŸ”„ LoRA ì„¤ì • ì ìš© ì¤‘...")
            model = FastLanguageModel.get_peft_model(
                model,
                r=lora_r,
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                              "gate_proj", "up_proj", "down_proj"],
                lora_alpha=lora_alpha,
                lora_dropout=lora_dropout,
                bias="none",
                use_gradient_checkpointing="unsloth",
                random_state=3407,
                use_rslora=False,
                loftq_config=None,
            )
            print("âœ… LoRA ì„¤ì • ì™„ë£Œ!")
        
        return model, tokenizer
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


# ==================== í•™ìŠµ ====================
def train_model(
    model,
    tokenizer,
    dataset: Dataset,
    max_seq_length: int = 2048,
    output_dir: str = "outputs",
    per_device_train_batch_size: int = 2,
    gradient_accumulation_steps: int = 4,
    max_steps: int = 60,
    learning_rate: float = 2e-4,
    warmup_steps: int = 5,
):
    """ëª¨ë¸ í•™ìŠµ"""
    print("\nğŸš€ í•™ìŠµ ì‹œì‘...")
    
    # ë°ì´í„°ì…‹ í¬ë§·íŒ…
    formatted_dataset = dataset.map(
        lambda x: formatting_prompts_func(x, tokenizer),
        batched=True
    )
    
    print("\n[í¬ë§·íŒ…ëœ ë°ì´í„° ì˜ˆì‹œ]:")
    print(formatted_dataset[0]['text'][:500] + "...")
    
    # í•™ìŠµ ì„¤ì •
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=formatted_dataset,
        dataset_text_field="text",
        max_seq_length=max_seq_length,
        dataset_num_proc=2,
        packing=False,
        args=TrainingArguments(
            per_device_train_batch_size=per_device_train_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            warmup_steps=warmup_steps,
            max_steps=max_steps,
            learning_rate=learning_rate,
            fp16=not is_bfloat16_supported(),
            bf16=is_bfloat16_supported(),
            logging_steps=1,
            optim="adamw_8bit",
            weight_decay=0.01,
            lr_scheduler_type="linear",
            seed=3407,
            output_dir=output_dir,
            report_to="none",
        ),
    )
    
    # í•™ìŠµ ì‹¤í–‰
    trainer_stats = trainer.train()
    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    
    return trainer, trainer_stats


# ==================== ë©”ì¸ í•¨ìˆ˜ ====================
def main():
    parser = argparse.ArgumentParser(description="Qwen3 ë²ˆì—­ ëª¨ë¸ LoRA íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--model_size",
        type=str,
        default="0.6B",
        choices=list(AVAILABLE_MODELS.keys()),
        help="ëª¨ë¸ í¬ê¸° ì„ íƒ (0.6B, 1.7B, 4B, 8B, 32B)"
    )
    parser.add_argument(
        "--data_path",
        type=str,
        default="/data1/vivamine/study/data/data_v4.json",
        help="í•™ìŠµ ë°ì´í„° íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="outputs",
        help="ëª¨ë¸ ì¶œë ¥ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--max_steps",
        type=int,
        default=60,
        help="ìµœëŒ€ í•™ìŠµ ìŠ¤í… ìˆ˜"
    )
    parser.add_argument(
        "--random_seed",
        type=int,
        default=42,
        help="ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42)"
    )
    
    args = parser.parse_args()
    
    model_name = AVAILABLE_MODELS[args.model_size]
    output_dir = os.path.join(args.output_dir, args.model_size.replace(".", "_"))
    
    print(f"\n{'='*60}")
    print(f"ëª¨ë¸: {model_name} ({args.model_size})")
    print(f"í•™ìŠµ ë°ì´í„°: {args.data_path}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"{'='*60}")
    
    # ì¬í˜„ì„± ì„¤ì •
    torch.manual_seed(args.random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.random_seed)

    # í•™ìŠµ ë°ì´í„° ë¡œë“œ
    train_dataset = load_dataset_jsonl(args.data_path)

    # ëª¨ë¸ ë¡œë“œ (LoRA ì ìš©)
    model, tokenizer = load_model(model_name=model_name, use_lora=True)

    # í•™ìŠµ ìˆ˜í–‰
    trainer, trainer_stats = train_model(
        model=model,
        tokenizer=tokenizer,
        dataset=train_dataset,
        output_dir=output_dir,
        max_steps=args.max_steps,
    )

    # ëª¨ë¸ ì €ì¥
    model.save_pretrained(os.path.join(output_dir, "model"))
    tokenizer.save_pretrained(os.path.join(output_dir, "tokenizer"))
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")
    print("\nâœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! (í‰ê°€ëŠ” eval_qwen3_translation.pyì—ì„œ ìˆ˜í–‰í•˜ì„¸ìš”)")


if __name__ == "__main__":
    main()

