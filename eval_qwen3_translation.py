#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3 ë²ˆì—­ ëª¨ë¸ ì´ˆì›” ë²ˆì—­ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì—­í• :
- NLP_testset.xlsxì˜ ì˜ì–´ ì›ë¬¸(Source Text)ì„ seed=42ë¡œ 50ê°œ ìƒ˜í”Œë§
- ê° ë¬¸ì¥ì„ Qwen3 (LoRA íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê¸°ì¤€)ë¡œ ë²ˆì—­
- ì—‘ì…€ì˜ ì´ˆì›”ë²ˆì—­ ì»¬ëŸ¼ê³¼ì˜ ìœ ì‚¬ë„ë¥¼ BLEU ì ìˆ˜ë¡œ ê³„ì‚°

í•™ìŠµ(íŒŒì¸íŠœë‹)ì€ train_qwen3_translation.pyì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import os
import argparse
import random
from typing import List, Dict

import torch
import numpy as np
import pandas as pd
import sacrebleu

from unsloth import FastLanguageModel

from train_qwen3_translation import (
    SYSTEM_PROMPT,
    ALPACA_PROMPT,
    AVAILABLE_MODELS,
    load_model,
)


def load_excel_samples(
    excel_path: str,
    source_col: str = "ì˜ì–´ ì›ë¬¸ (Source Text)",
    target_col: str = "ì´ˆì›” ë²ˆì—­",
    sample_size: int = 50,
    random_seed: int = 42,
) -> List[Dict[str, str]]:
    """ì—‘ì…€ì—ì„œ Source/Target ìŒì„ ìƒ˜í”Œë§í•´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    if not os.path.exists(excel_path):
        raise FileNotFoundError(f"âŒ ì—‘ì…€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {excel_path}")

    # ëª¨ë“  random seed ì„¤ì • (ì¬í˜„ì„± ë³´ì¥)
    random.seed(random_seed)
    np.random.seed(random_seed)
    torch.manual_seed(random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(random_seed)

    df = pd.read_excel(excel_path)

    if source_col not in df.columns or target_col not in df.columns:
        raise ValueError(
            f"âŒ ì—‘ì…€ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
            f"ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼: {list(df.columns)}, "
            f"í•„ìš”í•œ ì»¬ëŸ¼: source_col='{source_col}', target_col='{target_col}'"
        )

    n = len(df)
    if n == 0:
        raise ValueError("âŒ ì—‘ì…€ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    if sample_size >= n:
        sampled_df = df.copy()
        print(f"âš ï¸ ìƒ˜í”Œ ê°œìˆ˜ {sample_size} >= ì „ì²´ í–‰ {n}, ì „ì²´ í–‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        # pandasì˜ sample() ë©”ì„œë“œ ì‚¬ìš© (ë” ì¼ê´€ëœ ê²°ê³¼)
        sampled_df = df.sample(n=sample_size, random_state=random_seed)

    samples: List[Dict[str, str]] = []
    for idx, row in sampled_df.iterrows():
        src = str(row[source_col]).strip()
        tgt = str(row[target_col]).strip()
        if not src or not tgt:
            continue
        samples.append(
            {
                "id": int(idx),
                "source": src,
                "target": tgt,
            }
        )

    print(f"ğŸ“Š ì—‘ì…€ì—ì„œ {len(samples)}ê°œ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ (ì´ í–‰ {n}, seed={random_seed})")
    return samples


def generate_translation(
    model,
    tokenizer,
    text: str,
    instruction: str = "ì§ì—­í•˜ì§€ë§ê³  íƒ€ê²Ÿì–¸ì–´ ë¬¸í™”ê¶Œì— ë§ê²Œ ë²ˆì—­í•´ì¤˜",
    max_new_tokens: int = 128,
) -> str:
    """ë‹¨ì¼ ë¬¸ì¥ ë²ˆì—­"""
    FastLanguageModel.for_inference(model)
    eos_token = tokenizer.eos_token

    prompt = ALPACA_PROMPT.format(
        system_prompt=SYSTEM_PROMPT,
        instruction=instruction,
        input=text,
        output="",
    )

    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        use_cache=True,
    )
    result = tokenizer.batch_decode(outputs)

    try:
        translated_text = (
            result[0].split("### Output:\n")[1].replace(eos_token, "").strip()
        )
    except IndexError:
        translated_text = result[0].replace(eos_token, "").strip()

    return translated_text


def evaluate_model_on_excel(
    model,
    tokenizer,
    samples: List[Dict[str, str]],
    model_name: str,
) -> Dict:
    """ì—‘ì…€ ìƒ˜í”Œë“¤ì— ëŒ€í•´ ë²ˆì—­/ìœ ì‚¬ë„ í‰ê°€"""
    predictions: List[str] = []
    references: List[str] = []
    detailed: List[Dict[str, str]] = []

    print(f"\nâœ… ëª¨ë¸ í‰ê°€ ì‹œì‘: {model_name}")
    print("=" * 60)

    for i, sample in enumerate(samples):
        src = sample["source"]
        tgt = sample["target"]

        pred = generate_translation(model, tokenizer, src)

        predictions.append(pred)
        references.append(tgt)

        detailed.append(
            {
                "id": sample["id"],
                "source": src,
                "target": tgt,
                "prediction": pred,
            }
        )

        if i < 5:
            print(f"[{i+1}] Source: {src}")
            print(f"    Target(ì´ˆì›”ë²ˆì—­): {tgt}")
            print(f"    Prediction: {pred}")
            print("-" * 60)

    bleu = sacrebleu.corpus_bleu(predictions, [references])
    print(f"\nğŸ“Š BLEU ì ìˆ˜: {bleu.score:.2f}")

    return {
        "model_name": model_name,
        "bleu": bleu.score,
        "detailed": detailed,
    }


def main():
    parser = argparse.ArgumentParser(description="Qwen3 ì´ˆì›” ë²ˆì—­ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--model_size",
        type=str,
        default="0.6B",
        choices=list(AVAILABLE_MODELS.keys()),
        help="ë‹¨ì¼ ëª¨ë¸ í¬ê¸° ì„ íƒ (0.6B, 1.7B, 4B, 8B, 32B)",
    )
    parser.add_argument(
        "--compare_models",
        nargs="+",
        help="ì—¬ëŸ¬ ëª¨ë¸ í¬ê¸°ë¥¼ ë¹„êµ í‰ê°€ (ì˜ˆ: --compare_models 0.6B 1.7B 4B)",
    )
    parser.add_argument(
        "--excel_path",
        type=str,
        default="/data1/vivamine/study/data/NLP_testset.xlsx",
        help="í‰ê°€ìš© ì—‘ì…€ íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--source_col",
        type=str,
        default="ì˜ì–´ ì›ë¬¸ (Source Text)",
        help="ì˜ì–´ ì›ë¬¸ ì»¬ëŸ¼ ì´ë¦„",
    )
    parser.add_argument(
        "--target_col",
        type=str,
        default="ì´ˆì›” ë²ˆì—­",
        help="ì´ˆì›” ë²ˆì—­(ë ˆí¼ëŸ°ìŠ¤) ì»¬ëŸ¼ ì´ë¦„",
    )
    parser.add_argument(
        "--sample_size",
        type=int,
        default=50,
        help="ìƒ˜í”Œë§í•  ë¬¸ì¥ ìˆ˜",
    )
    parser.add_argument(
        "--random_seed",
        type=int,
        default=42,
        help="ìƒ˜í”Œë§ ëœë¤ ì‹œë“œ",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="outputs_eval",
        help="í‰ê°€ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬",
    )

    args = parser.parse_args()

    # ì‚¬ìš©í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ê²°ì •
    if args.compare_models:
        model_sizes = args.compare_models
    else:
        model_sizes = [args.model_size]

    # ì—‘ì…€ì—ì„œ í‰ê°€ ìƒ˜í”Œ ë¡œë“œ
    samples = load_excel_samples(
        excel_path=args.excel_path,
        source_col=args.source_col,
        target_col=args.target_col,
        sample_size=args.sample_size,
        random_seed=args.random_seed,
    )

    os.makedirs(args.output_dir, exist_ok=True)

    for size in model_sizes:
        if size not in AVAILABLE_MODELS:
            print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í¬ê¸°ì…ë‹ˆë‹¤: {size} (ê±´ë„ˆëœ€)")
            continue

        base_model_name = AVAILABLE_MODELS[size]
        model_output_dir = os.path.join("outputs", size.replace(".", "_"), "model")

        print(f"\n{'='*60}")
        print(f"ëª¨ë¸ í¬ê¸°: {size}")
        print(f"ë² ì´ìŠ¤ ëª¨ë¸: {base_model_name}")
        print(f"LoRA ê°€ì¤‘ì¹˜ ë””ë ‰í† ë¦¬: {model_output_dir}")
        print(f"{'='*60}")

        # ë² ì´ìŠ¤ ëª¨ë¸ + LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
        model, tokenizer = load_model(model_name=base_model_name, use_lora=False)

        if os.path.exists(model_output_dir):
            from peft import PeftModel

            model = PeftModel.from_pretrained(model, model_output_dir)
            print("âœ… í•™ìŠµëœ LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
        else:
            print("âš ï¸ í•™ìŠµëœ LoRA ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë² ì´ìŠ¤ ëª¨ë¸ë¡œë§Œ í‰ê°€í•©ë‹ˆë‹¤.")

        result = evaluate_model_on_excel(
            model=model,
            tokenizer=tokenizer,
            samples=samples,
            model_name=base_model_name,
        )

        # ê²°ê³¼ ì €ì¥
        import json

        save_path = os.path.join(
            args.output_dir, f"eval_results_{size.replace('.', '_')}.json"
        )
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {save_path}")

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model, tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    print("\nâœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")


if __name__ == "__main__":
    main()


